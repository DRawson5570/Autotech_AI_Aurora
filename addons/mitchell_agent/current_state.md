# Current State ‚Äî Mitchell Agent (AI Employee)

**Last updated:** 2026-01-12

## Summary üéØ
This document records the current project state: what we implemented recently, why, and what I'm actively working on now.

---

## Recent Fixes (2026-01-12) üîß

### Fixed: "list index out of range" Crash in Vehicle Selection
**Root Cause:** When the vehicle selector showed a "Submodel" tab but the submodel values list was empty, the code crashed with `IndexError: list index out of range`.

**Fix:** Added proper empty list checks in:
- `agent/navigator.py` ‚Äî Changed `else:` to `elif len(submodel_values) > 1:` and added fallback for empty lists
- `agent/request_handler.py` ‚Äî Added safety check before accessing `missing_info[0]`
- `agent/service.py` ‚Äî Added safety check before accessing `missing_info[0]`

### Added: Gemini API Retry Logic for Rate Limiting
**Problem:** Gemini API returns 503 "overloaded" errors during high load, causing ~10-20% of requests to fail.

**Solution:** Added exponential backoff retry in `ai_navigator/an_models.py`:
- Up to 3 retries for 503 errors
- Delays: 2s, 4s, 8s (exponential backoff)
- Logs retry attempts for debugging

### Verification
- Direct Gemini API test: **100% pass rate** (10/10) ‚Äî proves LLM correctly handles images
- Open WebUI test after fix: **90%+ pass rate** with 15-20s delays between requests
- No more "list index out of range" errors in test runs

---

## Current Focus: AI-Driven Navigation Testing üß™

### What We're Testing
Using a local LLM (Llama 3.1 8B via Ollama) to intelligently navigate ShopKeyPro and extract automotive data.

### Test Setup
1. **Chrome**: Running with `--remote-debugging-port=9222`
2. **State**: Logged into ShopKeyPro, vehicle selected, at **landing page** (Quick Access buttons visible)
3. **Control**: We connect via CDP - we do NOT start/stop Chrome or handle login

### Critical Rules ‚ö†Ô∏è
- **NEVER use browser back** - can navigate away from ShopKeyPro entirely
- **NEVER logout** - caller is responsible for session management
- **NEVER kill Chrome** - user manages the browser
- Return to landing by **closing modals only**

### Test Flow
```
1. Connect to Chrome via CDP (port 9222)
2. AI navigates from landing page to find data
3. AI extracts data when found
4. AI closes modal to return to landing page
5. Verify back at landing page
6. DONE - Chrome stays logged in for next test
```

### What's Working ‚úÖ
- Fluid capacities - known path replay works
- Torque specs - exploration + learning works
- Return to landing page via modal close
- Ollama 8B model makes good decisions

### Recent Fixes (2026-01-10)
- Removed ALL `go_back()` calls - too dangerous
- Fixed `extract_data` to return immediately when data found
- Added `_return_to_landing_page()` - modal close only
- Fixed `backtrack_to_landing()` - modal close only

---

## Background / Goal
- Build an autonomous "AI Employee" that runs at customer sites and retrieves automotive technical data from ShopKeyPro.
- Use an LLM (Ollama) to *navigate* the ShopKeyPro vehicle selector step-by-step and extract data via tools.
- Support customers who do not have a GPU by providing a server-side navigation fallback.
- **Securely bill users** for server-side navigation without allowing agents to bypass billing.

---

## Recent Work (Completed) ‚úÖ
- Implemented local Ollama navigator with native tool calling (`qwen3:8b`) that outputs structured tool calls (e.g., `select_year`, `request_info`, `confirm_vehicle`).
- Added clarification flow (`request_info`) where navigator asks Autotech AI for missing option values and waits for an answer from the user via the server.
- Cleaned up `.env` and `.env.example` and adopted the "AI Employee" configuration style.
- Created server-side navigation brain (`server/navigation.py`) that runs Ollama on the server and returns a navigation action for clients without local models.
- Added `POST /api/mitchell/navigate` endpoint and corresponding Pydantic models (`NavigationRequest`, `NavigationResponse`, `NavigationAction`, plus token usage fields).
- Added hybrid navigation mode to the client-side navigator (`agent/navigator.py`):
  - `NavigationMode` enum: `auto` (default), `local`, `server`
  - Auto mode tries local Ollama first and falls back to the server automatically
  - `call_server()` implemented to call server `/navigate` endpoint
- Updated `agent/config.py` and `.env` to include `NAVIGATION_MODE` and pass it through the service.
- Updated `agent/service.py` to initialize navigator with `navigation_mode`, `shop_id`, and set `request_id` per-request for billing lookup.
- **Implemented secure billing for server navigation:**
  - Agent sends `request_id` (not `user_id`) to prevent spoofing
  - Server looks up `request_id` ‚Üí finds original `user_id` who created the request
  - Server bills that verified user per navigation step
  - Token usage extracted from Ollama response and recorded via `record_usage_event()`
  - Billing is non-blocking (errors logged but don't fail navigation)
- Updated Open WebUI tool to pass `user_id` from `__user__` context and include in queued requests
- Updated request/queue flow to persist `user_id` in `MitchellRequest` model
- Verified `record_usage_event` already decrements `UserTokenBalance` and aggregates usage monthly

---

## Security Model üîí
**Problem:** Agents could spoof `user_id` to bypass billing.

**Solution:** 
1. Tool creates request with verified `user_id` from Open WebUI context
2. Request stored in queue: `request_id` ‚Üí `user_id` mapping
3. Agent only knows `request_id`, sends it to `/navigate`
4. Server looks up `request_id` to find original `user_id`
5. Server bills that verified user
6. Agent cannot manipulate billing

---

## Pending / Next Tasks (short-term) üìù
1. Add **unit tests** and **integration tests** to verify:
   - Server navigation returns `tokens_used` and `record_usage_event` is called
   - Agent sends `request_id` correctly
   - Server looks up user_id and bills correctly
   - Billing cannot be bypassed by spoofing request_id
2. Add observability/metrics: record per-step tokens, total per-job tokens
3. Update admin UI (billing pages) to show Mitchell navigation token usage with `token_source="mitchell_navigation"` filter
4. Add docs for operators: how to configure `NAVIGATION_MODE`, monitor billing, troubleshoot failures

---

## Files Touched (representative) üìÅ
- agent/
  - `navigator.py` ‚Äî Hybrid navigation, sends `request_id` to server
  - `service.py` ‚Äî Sets `navigator.request_id` per request
  - `config.py` ‚Äî Added `navigation_mode` setting
- server/
  - `navigation.py` ‚Äî Extracts token usage from Ollama, returns `NavigationDecision` with tokens
  - `router.py` ‚Äî `/navigate` looks up user_id from request_id, bills verified user
  - `models.py` ‚Äî `NavigationRequest` uses `request_id`, added `TokenUsage` model
  - `queue.py` ‚Äî Persist `user_id` in requests
- openwebui_tool.py ‚Äî Passes `__user__` context, includes `user_id` in requests
- DEVELOPER_GUIDE.md ‚Äî Updated with hybrid architecture and security notes
- current_state.md ‚Äî This file

---

## How to Test / Verify ‚úÖ
- Manual: Run request with `NAVIGATION_MODE=server`, observe `/navigate` calls and `usage_event` records
- Unit tests: Mock `/navigate` call, assert `record_usage_event` invoked with correct user_id
- Security test: Try sending wrong request_id, verify server returns 404
- Integration: Full flow from tool ‚Üí queue ‚Üí agent ‚Üí server navigation ‚Üí billing

---

## Design Notes / Decisions üí°
- Billing per transaction (not per job) because job boundaries aren't well-defined
- Security: request_id is the only trusted identifier agents can provide
- Billing is non-blocking: errors logged but navigation continues
- `NAVIGATION_MODE=auto` recommended: local GPU users get free navigation, others billed transparently

---

## Current Status ‚úÖ
All implementation complete. System is secure and ready for testing.